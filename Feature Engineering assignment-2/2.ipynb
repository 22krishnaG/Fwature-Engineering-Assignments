{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE ENGINEERING - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Min-Max scaling, also known as feature scaling or normalization, is a data preprocessing technique used to transform numerical features in a dataset into a common range. The goal of Min-Max scaling is to bring all the feature values within a specified range, typically between 0 and 1. This scaling method is particularly useful when features have different scales and ranges, and you want to ensure that all features contribute equally to the analysis or modeling process.\n",
    "\n",
    "The Min-Max scaling formula for a feature X is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = (X - X_min) / (X_max - X_min)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "X is the original feature value.\n",
    "X_min is the minimum value of the feature in the dataset.\n",
    "X_max is the maximum value of the feature in the dataset.\n",
    "Min-Max scaling preserves the relative relationships between feature values while bringing them within the desired range.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a dataset with a feature representing house prices and another feature representing the number of bedrooms. The house prices range from $100,000 to $1,000,000, and the number of bedrooms ranges from 1 to 5.\n",
    "\n",
    "Original data:\n",
    "\n",
    "House Price: $100,000, $500,000, $750,000, $1,000,000\n",
    "Number of Bedrooms: 1, 2, 4, 5\n",
    "To apply Min-Max scaling:\n",
    "\n",
    "Calculate the minimum and maximum values for each feature.\n",
    "\n",
    "Min(House Price) = $100,000\n",
    "Max(House Price) = $1,000,000\n",
    "Min(Number of Bedrooms) = 1\n",
    "Max(Number of Bedrooms) = 5\n",
    "Apply the scaling formula for each data point:\n",
    "\n",
    "Scaled House Price = ($100,000 - $100,000) / ($1,000,000 - $100,000) = 0.0\n",
    "\n",
    "Scaled House Price = ($500,000 - $100,000) / ($1,000,000 - $100,000) = 0.4\n",
    "\n",
    "Scaled House Price = ($750,000 - $100,000) / ($1,000,000 - $100,000) = 0.65\n",
    "\n",
    "Scaled House Price = ($1,000,000 - $100,000) / ($1,000,000 - $100,000) = 1.0\n",
    "\n",
    "Scaled Number of Bedrooms = (1 - 1) / (5 - 1) = 0.0\n",
    "\n",
    "Scaled Number of Bedrooms = (2 - 1) / (5 - 1) = 0.25\n",
    "\n",
    "Scaled Number of Bedrooms = (4 - 1) / (5 - 1) = 0.75\n",
    "\n",
    "Scaled Number of Bedrooms = (5 - 1) / (5 - 1) = 1.0\n",
    "\n",
    "The transformed dataset after Min-Max scaling:\n",
    "\n",
    "Scaled House Price: 0.0, 0.4, 0.65, 1.0\n",
    "Scaled Number of Bedrooms: 0.0, 0.25, 0.75, 1.0\n",
    "Min-Max scaling ensures that both features are now within the range of 0 to 1, making them comparable in terms of scale. This is particularly important for machine learning algorithms that are sensitive to the scale of features, such as gradient descent-based methods and k-nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "The Unit Vector technique, also known as vector normalization or feature scaling by the L2 norm, is a data preprocessing method used to scale features by dividing each feature vector by its magnitude (L2 norm). This technique ensures that each feature vector has a unit norm, meaning it has a length of 1. It's particularly useful when you want to emphasize the direction of the data rather than its magnitude.\n",
    "\n",
    "The formula to calculate the unit vector for a feature vector X is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unit = X / ||X||\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where:\n",
    "\n",
    "X is the original feature vector.\n",
    "||X|| represents the L2 norm (Euclidean norm) of the feature vector, calculated as the square root of the sum of squared values of the elements in the vector.\n",
    "Unlike Min-Max scaling, which brings all features within a common range, the Unit Vector technique focuses on the direction of the feature vectors while maintaining their relative magnitudes.\n",
    "\n",
    "Difference from Min-Max Scaling:\n",
    "\n",
    "Effect on Magnitude:\n",
    "\n",
    "Min-Max Scaling: Scales the feature values to fit within a specified range, often between 0 and 1.\n",
    "Unit Vector Technique: Scales the feature vectors to have a length of 1 (unit norm), preserving the direction of the data.\n",
    "Use Case:\n",
    "\n",
    "Min-Max Scaling: Useful when you want to bring all features to a common scale for comparison and analysis.\n",
    "Unit Vector Technique: Useful when you're interested in the relationships between feature vectors and their directions.\n",
    "Example:\n",
    "\n",
    "Consider a dataset with two features representing exam scores: Math and English. Each data point is a student's scores.\n",
    "\n",
    "Original data:\n",
    "\n",
    "Math: 90, 80, 70, 85\n",
    "English: 95, 85, 75, 90\n",
    "To apply the Unit Vector technique:\n",
    "\n",
    "Calculate the L2 norm (Euclidean norm) for each data point:\n",
    "\n",
    "L2 Norm (Data Point 1) = sqrt(90^2 + 95^2) = 132.68\n",
    "L2 Norm (Data Point 2) = sqrt(80^2 + 85^2) = 114.02\n",
    "L2 Norm (Data Point 3) = sqrt(70^2 + 75^2) = 103.85\n",
    "L2 Norm (Data Point 4) = sqrt(85^2 + 90^2) = 127.08\n",
    "Apply the Unit Vector formula for each data point:\n",
    "\n",
    "Unit Vector (Data Point 1) = [90/132.68, 95/132.68] = [0.6773, 0.7200]\n",
    "Unit Vector (Data Point 2) = [80/114.02, 85/114.02] = [0.7014, 0.7127]\n",
    "Unit Vector (Data Point 3) = [70/103.85, 75/103.85] = [0.6739, 0.7383]\n",
    "Unit Vector (Data Point 4) = [85/127.08, 90/127.08] = [0.6690, 0.7431]\n",
    "The transformed dataset after applying the Unit Vector technique:\n",
    "\n",
    "Unit Vector (Data Point 1): [0.6773, 0.7200]\n",
    "Unit Vector (Data Point 2): [0.7014, 0.7127]\n",
    "Unit Vector (Data Point 3): [0.6739, 0.7383]\n",
    "Unit Vector (Data Point 4): [0.6690, 0.7431]\n",
    "The Unit Vector technique ensures that each data point's vector has a length of 1, emphasizing the directions of the vectors while maintaining their relationships.\n",
    "\n",
    "In summary, while Min-Max Scaling aims to bring features within a common range, the Unit Vector technique emphasizes the direction of feature vectors by scaling them to have a unit norm (length of 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis):\n",
    "PCA is a dimensionality reduction technique used to transform high-dimensional data into a lower-dimensional representation while preserving as much of the original data's variance as possible. It does this by identifying the directions (principal components) along which the data varies the most. These principal components are orthogonal to each other and capture the most significant information in the data.\n",
    "\n",
    "How PCA is used in Dimensionality Reduction:\n",
    "PCA is used to reduce the dimensionality of a dataset while retaining as much relevant information as possible. It does this by projecting the original data points onto a new coordinate system defined by the principal components. The first principal component captures the direction of maximum variance, the second principal component captures the direction of second maximum variance, and so on. By selecting a subset of the principal components, you can create a lower-dimensional representation of the data.\n",
    "\n",
    "Example: Using PCA for Dimensionality Reduction in Python:\n",
    "Let's illustrate PCA's application for dimensionality reduction using a sample dataset in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      "[[ 2  4]\n",
      " [ 3  6]\n",
      " [ 4  8]\n",
      " [ 5 10]\n",
      " [ 6 12]]\n",
      "\n",
      "Reduced Features (Principal Component):\n",
      "[[ 4.47213595]\n",
      " [ 2.23606798]\n",
      " [-0.        ]\n",
      " [-2.23606798]\n",
      " [-4.47213595]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data: A 2D dataset with two features\n",
    "data = np.array([[2, 4],\n",
    "                 [3, 6],\n",
    "                 [4, 8],\n",
    "                 [5, 10],\n",
    "                 [6, 12]])\n",
    "\n",
    "# Initialize PCA with desired number of components\n",
    "pca = PCA(n_components=1)\n",
    "\n",
    "# Fit PCA on the data and transform it\n",
    "reduced_features = pca.fit_transform(data)\n",
    "\n",
    "# Print the original data and the reduced features\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nReduced Features (Principal Component):\")\n",
    "print(reduced_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we have a 2D dataset with two features for each data point. We want to reduce the dimensionality to one dimension using PCA.\n",
    "\n",
    "Import the necessary libraries, including PCA from sklearn.decomposition.\n",
    "\n",
    "Define your sample data as a NumPy array.\n",
    "\n",
    "Initialize the PCA object with the desired number of components (n_components=1 for 1D reduction).\n",
    "\n",
    "Fit the PCA on the data and transform it. The fit_transform method computes the principal components and projects the data onto the reduced-dimensional space.\n",
    "\n",
    "Print both the original data and the reduced features (principal component).\n",
    "\n",
    "In this case, the reduced features represent the transformed data in a lower-dimensional space. The first principal component captures the direction of maximum variance in the original data. By using PCA, you can achieve dimensionality reduction while retaining most of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Sample data: five data points with three features each\n",
    "data = np.array([[2, 3, 1],\n",
    "                 [4, 1, 5],\n",
    "                 [6, 7, 2],\n",
    "                 [8, 5, 7],\n",
    "                 [10, 9, 3]])\n",
    "\n",
    "# Initialize PCA with desired number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA on the data and transform it\n",
    "reduced_features = pca.fit_transform(data)\n",
    "\n",
    "# Print the original data and the reduced features\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nReduced Features (Principal Components):\")\n",
    "print(reduced_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "n this example, we'll perform PCA for feature extraction using the PCA class from scikit-learn. Here's a breakdown of the code:\n",
    "\n",
    "Import the necessary libraries, including PCA from sklearn.decomposition.\n",
    "\n",
    "Define your sample data as a NumPy array. Each row represents a data point, and each column represents a feature.\n",
    "\n",
    "Initialize the PCA object with the desired number of components (n_components=2). This means you want to extract two principal components.\n",
    "\n",
    "Fit the PCA on the data and transform it. The fit_transform method takes your data and returns the transformed reduced features.\n",
    "\n",
    "Print the original data and the reduced features (principal components).\n",
    "\n",
    "In this case, you are reducing the three original features to two principal components. The first principal component captures the direction of maximum variance in the data, and the second principal component captures the second-highest variance direction orthogonal to the first principal component.\n",
    "\n",
    "The reduced features (principal components) in the output represent the transformed data with lower dimensionality. These transformed features are linear combinations of the original features and are chosen to retain the maximum variance present in the original data.\n",
    "\n",
    "In summary, PCA is a technique for feature extraction that allows you to reduce the dimensionality of your data while preserving its important information. The example above demonstrates how to perform PCA for feature extraction in Python using the scikit-learn library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data: features for different food items\n",
    "data = np.array([[10, 4.5, 30],\n",
    "                 [20, 3.7, 45],\n",
    "                 [15, 4.2, 25],\n",
    "                 [25, 4.9, 40]])\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data using Min-Max scaling\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Print the scaled data\n",
    "print(\"Original Data:\")\n",
    "print(data)\n",
    "print(\"\\nScaled Data after Min-Max Scaling:\")\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we're using the MinMaxScaler class from scikit-learn to apply Min-Max scaling to the food delivery dataset. Here's what the code does:\n",
    "\n",
    "Import the necessary libraries, including MinMaxScaler from sklearn.preprocessing.\n",
    "\n",
    "Define your sample data as a NumPy array. Each row represents a food item, and each column represents a feature (price, rating, delivery time).\n",
    "\n",
    "Initialize the MinMaxScaler object.\n",
    "\n",
    "Fit and transform the data using Min-Max scaling. The fit_transform method computes the minimum and maximum values for each feature and scales the features accordingly.\n",
    "\n",
    "Print both the original data and the scaled data.\n",
    "\n",
    "When you run this code, you'll notice that the scaled data will have all features scaled between 0 and 1, preserving the relationships between the features while ensuring that they are on a similar scale. This preprocessing step is important for building a recommendation system because it prevents features with larger numerical values from dominating the analysis, ensuring that all features contribute evenly to the system's recommendations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Using Principal Component Analysis (PCA) for dimensionality reduction in the context of building a stock price prediction model involves the following steps:\n",
    "\n",
    "Data Collection and Preprocessing:\n",
    "\n",
    "Gather the dataset containing various features, such as company financial data (e.g., revenue, profit) and market trends (e.g., trading volume, sector performance).\n",
    "Preprocess the data by handling missing values, scaling numerical features, and encoding categorical variables if necessary.\n",
    "Standardization:\n",
    "\n",
    "Before applying PCA, it's recommended to standardize the data so that all features have zero mean and unit variance. This is important because PCA is sensitive to the scale of features.\n",
    "PCA Application:\n",
    "\n",
    "Apply PCA to the standardized data. The PCA algorithm calculates the principal components (linear combinations of the original features) that capture the most significant variance in the data.\n",
    "Variance Explained:\n",
    "\n",
    "Analyze the explained variance ratio to determine the number of principal components to retain. This ratio tells you the proportion of the total variance that each principal component explains.\n",
    "Selecting the Number of Components:\n",
    "\n",
    "Decide on the number of principal components to keep based on the explained variance ratio. A common approach is to choose the number of components that explain a sufficiently high percentage of the total variance (e.g., 95% or 99%).\n",
    "Dimensionality Reduction:\n",
    "\n",
    "Transform the original data using the selected number of principal components. This step reduces the dimensionality of the dataset while preserving the most significant information.\n",
    "Model Building:\n",
    "\n",
    "Use the reduced-dimension dataset as input to your stock price prediction model. You can employ various machine learning algorithms, such as regression, time-series models, or neural networks, depending on the nature of the problem.\n",
    "Benefits of Using PCA for Stock Price Prediction Models:\n",
    "\n",
    "Reduced Overfitting: By reducing the dimensionality of the dataset, PCA can help prevent overfitting, which is especially important in complex modeling tasks like stock price prediction.\n",
    "\n",
    "Noise Reduction: PCA can help remove noise and irrelevant features, focusing on the most informative dimensions.\n",
    "\n",
    "Interpretability: In some cases, principal components might have a more meaningful interpretation than the original features, simplifying the model's explanation.\n",
    "\n",
    "Computation Efficiency: Fewer features can lead to faster training and prediction times, which is beneficial when working with large datasets.\n",
    "\n",
    "Keep in mind that while PCA can offer advantages, it's important to carefully consider its application. The trade-off between dimensionality reduction and loss of information should be assessed, as well as the potential impact on the interpretability of the model's predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: To perform Min-Max scaling and transform the values to a range of -1 to 1, you need to apply the Min-Max scaling formula. Here's how you can do it for the given dataset:\n",
    "\n",
    "Calculate the minimum and maximum values of the original dataset.\n",
    "Apply the Min-Max scaling formula to each value in the dataset.\n",
    "Let's perform these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data: [ 1  5 10 15 20]\n",
      "Scaled Data (-1 to 1): [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "# Calculate the minimum and maximum values\n",
    "data_min = np.min(data)\n",
    "data_max = np.max(data)\n",
    "\n",
    "# Define the desired range for scaling (-1 to 1)\n",
    "scaled_min = -1\n",
    "scaled_max = 1\n",
    "\n",
    "# Apply Min-Max scaling\n",
    "scaled_data = scaled_min + (data - data_min) * (scaled_max - scaled_min) / (data_max - data_min)\n",
    "\n",
    "print(\"Original Data:\", data)\n",
    "print(\"Scaled Data (-1 to 1):\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, the original values [1, 5, 10, 15, 20] are scaled to the range of -1 to 1 using Min-Max scaling. The resulting scaled values are [-1.0, -0.5, 0.0, 0.5, 1.0], as shown in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: To perform feature extraction using PCA on the given dataset, you need to follow these steps:\n",
    "\n",
    "1. Standardize the features: Before applying PCA, it's important to standardize the features to ensure that they have zero mean and unit variance.\n",
    "\n",
    "2. Apply PCA: Calculate the principal components of the standardized features.\n",
    "\n",
    "3. Analyze explained variance: Examine the explained variance ratio associated with each principal component. This ratio tells you the proportion of the total variance that each component explains.\n",
    "\n",
    "4. Decide on the number of components: Choose the number of principal components to retain based on the cumulative explained variance ratio and your desired level of retained information.\n",
    "\n",
    "Since you haven't provided the actual data and its characteristics, I'll give you a general guideline for choosing the number of principal components to retain:\n",
    "\n",
    "Calculate the Cumulative Explained Variance:\n",
    "\n",
    "Calculate the cumulative sum of the explained variance ratios of the principal components.\n",
    "This will help you understand how much of the total variance in the dataset is captured by the first N principal components.\n",
    "\n",
    "Choose the Number of Components:\n",
    "\n",
    "Choose the number of principal components that collectively capture a sufficiently high percentage of the total variance.\n",
    "A common guideline is to aim for a cumulative explained variance of 95% or higher.\n",
    "\n",
    "Visualization (Optional):\n",
    "\n",
    "Visualize the explained variance ratios to see how quickly the cumulative explained variance increases as you add more components.\n",
    "This can provide insights into the intrinsic dimensionality of your data.\n",
    "\n",
    "Given the nature of the features (height, weight, age, gender, blood pressure), it's likely that the first few principal components will capture the most significant patterns in the data, potentially reducing the dimensionality while retaining most of the relevant information. However, the exact number of components to retain would depend on the variability of your specific dataset.\n",
    "\n",
    "Remember that the choice of the number of principal components can also be influenced by practical considerations, such as computational efficiency and interpretability. If you're unsure, you can experiment with different numbers of components and evaluate their impact on your downstream tasks, such as model performance in the case of prediction tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu4",
   "language": "python",
   "name": "gpu4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
